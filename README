What is the rough outline of what we're doing here:

The end result is: query a bunch of text and generate a prompt that returns paragraphs of text that are related to the query.

This requires the following:
1. Get a corpus to index.
2. Parse the text into chunks
3. Pass the text into openai ada embeddings and get a vector.
4. Write the embedding into a db with a mapping from embedding -> sentence chunk
5. Get a query string
6. Transform the query string into an embedding.
7. Run knn using the query embedding against the indexed data. 
8. Return top 5 sentence chunks.
9. Voila -> you have context for the query against the big boi LLM. 

Then you query the LLM with the following: Answer the follow query "<insert query text here>" given the following context { <sentence chunks from retrieval>}


